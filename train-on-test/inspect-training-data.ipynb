{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastian/anaconda3/envs/olmo-3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "1310619006 is out of bounds for dataset of size 934006730",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch_instances\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Get all 2048 x 2048 token IDs in the first batch.\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[43mget_batch_instances\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m, in \u001b[0;36mget_batch_instances\u001b[0;34m(batch_idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m batch_instances \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m batch_indices:\n\u001b[0;32m---> 24\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     25\u001b[0m     batch_instances\u001b[38;5;241m.\u001b[39mappend(token_ids)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_instances\n",
      "File \u001b[0;32m~/Documents/GitHub/OLMo/olmo/data/memmap_dataset.py:193\u001b[0m, in \u001b[0;36mMemMapDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    190\u001b[0m         memmap_local_index \u001b[38;5;241m=\u001b[39m pos_index \u001b[38;5;241m-\u001b[39m offset_start\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m memmap_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m memmap_local_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of bounds for dataset of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Read the data from file.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_chunk_from_memmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memmap_paths[memmap_index], memmap_local_index)\n",
      "\u001b[0;31mIndexError\u001b[0m: 1310619006 is out of bounds for dataset of size 934006730"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cached_path import cached_path\n",
    "\n",
    "from olmo.config import TrainConfig\n",
    "from olmo.data import build_memmap_dataset\n",
    "\n",
    "# Update these paths to what you want:\n",
    "data_order_file_path = cached_path(\"/home/sebastian/Downloads/global_indices.npy\")\n",
    "train_config_path = \"../configs/official/OLMo-7B.yaml\"\n",
    "\n",
    "\n",
    "cfg = TrainConfig.load(train_config_path)\n",
    "dataset = build_memmap_dataset(cfg, cfg.data)\n",
    "batch_size = cfg.global_train_batch_size\n",
    "global_indices = np.memmap(data_order_file_path, mode=\"r+\", dtype=np.uint32)\n",
    "\n",
    "\n",
    "def get_batch_instances(batch_idx: int) -> list[list[int]]:\n",
    "    batch_start = batch_idx * batch_size\n",
    "    batch_end = (batch_idx + 1) * batch_size\n",
    "    batch_indices = global_indices[batch_start:batch_end]\n",
    "    batch_instances = []\n",
    "    for index in batch_indices:\n",
    "        token_ids = dataset[index][\"input_ids\"].tolist()\n",
    "        batch_instances.append(token_ids)\n",
    "    return batch_instances\n",
    "\n",
    "\n",
    "# Get all 2048 x 2048 token IDs in the first batch.\n",
    "get_batch_instances(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olmo-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
