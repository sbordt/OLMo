{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from mlcloud import ferranti_exec, ferranti_print_logs\n",
    "\n",
    "def ferranti_exec_cpu(command):\n",
    "    sbatch_command = \"\"\"sbatch <<'EOF'\n",
    "#!/bin/bash\n",
    "#SBATCH --time=3-00:00:00  # Runtime in D-HH:MM:SS    \n",
    "#SBATCH --output=/weka/luxburg/sbordt10/logs/exec_cpu/%j.out  \n",
    "#SBATCH --error=/weka/luxburg/sbordt10/logs/exec_cpu/%j.err   \n",
    "#SBATCH --open-mode=append\n",
    "#SBATCH --job-name=ferranti-exec-cpu\n",
    "#SBATCH --partition=cpu-ferranti\n",
    "#SBATCH --nodes=1  \n",
    "#SBATCH --ntasks=1       \n",
    "#SBATCH --cpus-per-task=32    \n",
    "#SBATCH --mem=256G   \n",
    "\n",
    "scontrol show job ${SLURM_JOB_ID}\n",
    "export WANDB__SERVICE_WAIT=6000\n",
    "source activate tp-theory-new\n",
    "\"\"\"\n",
    "    ferranti_exec(sbatch_command + command + \"\\nEOF\")\n",
    "\n",
    "\n",
    "def ferranti_exec_H100(command):\n",
    "    sbatch_command = \"\"\"sbatch <<'EOF'\n",
    "#!/bin/bash\n",
    "#SBATCH --time=3-00:00:00  # Runtime in D-HH:MM:SS    \n",
    "#SBATCH --output=/weka/luxburg/sbordt10/logs/exec_h100/%j.out  \n",
    "#SBATCH --error=/weka/luxburg/sbordt10/logs/exec_h100/%j.err   \n",
    "#SBATCH --open-mode=append\n",
    "#SBATCH --job-name=ferranti-exec-h100\n",
    "#SBATCH --partition=h100-ferranti\n",
    "#SBATCH --nodes=1  \n",
    "#SBATCH --ntasks=1       \n",
    "#SBATCH --cpus-per-task=8    \n",
    "#SBATCH --mem=128G   \n",
    "#SBATCH --gres=gpu:1 \n",
    "\n",
    "scontrol show job ${SLURM_JOB_ID}\n",
    "nvidia-smi\n",
    "export NCCL_TIMEOUT=1800000\n",
    "export WANDB__SERVICE_WAIT=6000\n",
    "source activate tp-theory-new\n",
    "\"\"\"\n",
    "    ferranti_exec(sbatch_command + command + \"\\nEOF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor and control jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of logs/pythia-14m-mixed-lr-sweep/45282.out:\n",
      "JobId=45282 JobName=pythia-14m-mixed-lr-sweep\n",
      "   UserId=sbordt10(4198) GroupId=luxburg(4018) MCS_label=N/A\n",
      "   Priority=63351 Nice=0 Account=luxburg QOS=normal\n",
      "   JobState=RUNNING Reason=None Dependency=(null)\n",
      "   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n",
      "   RunTime=00:00:41 TimeLimit=3-00:00:00 TimeMin=N/A\n",
      "   SubmitTime=2025-03-17T15:45:13 EligibleTime=2025-03-17T15:45:13\n",
      "   AccrueTime=2025-03-17T15:45:13\n",
      "   StartTime=2025-03-17T15:45:13 EndTime=2025-03-20T15:45:13 Deadline=N/A\n",
      "   PreemptEligibleTime=2025-03-17T15:46:13 PreemptTime=None\n",
      "   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2025-03-17T15:45:13 Scheduler=Main\n",
      "   Partition=h100-ferranti AllocNode:Sid=ferranti-login001:1741294\n",
      "   ReqNodeList=(null) ExcNodeList=(null)\n",
      "   NodeList=mlcbm005\n",
      "   BatchHost=mlcbm005\n",
      "   NumNodes=1 NumCPUs=12 NumTasks=1 CPUs/Task=12 ReqB:S:C:T=0:0:*:*\n",
      "   ReqTRES=cpu=12,mem=128G,node=1,billing=102,gres/gpu=1\n",
      "   AllocTRES=cpu=12,mem=128G,node=1,billing=102,gres/gpu=1,gres/gpu:h100=1\n",
      "   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n",
      "   MinCPUsNode=12 MinMemoryNode=128G MinTmpDiskNode=0\n",
      "   Features=(null) DelayBoot=00:00:00\n",
      "   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n",
      "   Command=/weka/luxburg/sbordt10/limitations_of_tp_theory/scripts/ferranti/pythia-14m-mixed-lr-sweep.sh\n",
      "   WorkDir=/weka/luxburg/sbordt10/limitations_of_tp_theory/scripts/ferranti\n",
      "   StdErr=/weka/luxburg/sbordt10/logs/pythia-14m-mixed-lr-sweep/45282.err\n",
      "   StdIn=/dev/null\n",
      "   StdOut=/weka/luxburg/sbordt10/logs/pythia-14m-mixed-lr-sweep/45282.out\n",
      "   Power=\n",
      "   TresPerNode=gres/gpu:1\n",
      "   TresPerTask=cpu:12\n",
      "   \n",
      "\n",
      "Mon Mar 17 15:45:54 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  | 00000000:D7:00.0 Off |                    0 |\n",
      "| N/A   24C    P0              69W / 700W |      4MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "SLURM_PROCID: 0, SLURM_NTASKS: 1\n",
      "d_model:  4096\n",
      "n_head:  128\n",
      "d_head:  32\n",
      "n_layer:  6\n",
      "MLP intermediate size:  16384\n",
      "Number of model parameters:  1425294848\n",
      "{'data': {},\n",
      " 'devices': 'auto',\n",
      " 'eval': {'evaluate_example': 'first',\n",
      "          'final_validation': True,\n",
      "          'initial_validation': False,\n",
      "          'interval': 1000,\n",
      "          'max_iters': 100000,\n",
      "          'max_new_tokens': None},\n",
      " 'get_lr_fn': None,\n",
      " 'initial_checkpoint_dir': None,\n",
      " 'initialize_weights_fn': '<function initialize_weights at 0x1555544cc9a0>',\n",
      " 'logger_kwargs': None,\n",
      " 'logger_name': 'wandb',\n",
      " 'model_config': {'attention_logit_softcapping': None,\n",
      "                  'attention_scores_scalar': None,\n",
      "                  'attn_bias': False,\n",
      "                  'bias': True,\n",
      "                  'block_size': 512,\n",
      "                  'final_logit_softcapping': None,\n",
      "                  'gelu_approximate': 'none',\n",
      "                  'head_size': 32,\n",
      "                  'hf_config': {'name': 'pythia-14m', 'org': 'EleutherAI'},\n",
      "                  'intermediate_size': 16384,\n",
      "                  'lm_head_bias': False,\n",
      "                  'mlp_class_name': 'GptNeoxMLP',\n",
      "                  'n_embd': 4096,\n",
      "                  'n_expert': 0,\n",
      "                  'n_expert_per_token': 0,\n",
      "                  'n_head': 128,\n",
      "                  'n_layer': 6,\n",
      "                  'n_query_groups': 4,\n",
      "                  'name': 'pythia-14m',\n",
      "                  'norm_class_name': 'LayerNorm',\n",
      "                  'norm_eps': 1e-05,\n",
      "                  'padded_vocab_size': 50304,\n",
      "                  'padding_multiple': 128,\n",
      "                  'parallel_residual': True,\n",
      "                  'post_attention_norm': False,\n",
      "                  'post_mlp_norm': False,\n",
      "                  'rope_adjustments': None,\n",
      "                  'rope_base': 10000,\n",
      "                  'rope_condense_ratio': 1,\n",
      "                  'rotary_percentage': 0.25,\n",
      "                  'scale_embeddings': False,\n",
      "                  'shared_attention_norm': False,\n",
      "                  'sliding_window_layer_placing': None,\n",
      "                  'sliding_window_size': None,\n",
      "                  'vocab_size': 50254},\n",
      " 'model_name': None,\n",
      " 'num_nodes': 1,\n",
      " 'optimizer': \"{'class_path': 'torch.optim.AdamW', 'init_args': {'lr': 0.0001, \"\n",
      "              \"'betas': (0.9, 0.95), 'weight_decay': 0.1}}\",\n",
      " 'out_dir': '/weka/luxburg/sbordt10/moritz_sebastian_2025/moritz-pythia-14m-standard-width=4096-bf16mixed-lr=0.0001-warmup=700-timestamp=20250317_154603',\n",
      " 'precision': 'bf16-mixed',\n",
      " 'resume': False,\n",
      " 'seed': 42,\n",
      " 'tokenizer_dir': PosixPath('checkpoints/EleutherAI/pythia-14m'),\n",
      " 'train': {'epochs': None,\n",
      "           'global_batch_size': 512,\n",
      "           'log_interval': 1,\n",
      "           'lr_warmup_fraction': None,\n",
      "           'lr_warmup_steps': 700.0,\n",
      "           'max_norm': 1.0,\n",
      "           'max_seq_length': 512,\n",
      "           'max_steps': None,\n",
      "           'max_tokens': 7864320,\n",
      "           'micro_batch_size': 16,\n",
      "           'min_lr': 1e-05,\n",
      "           'save_interval': 1000,\n",
      "           'tie_embeddings': False},\n",
      " 'training_monitor': '<litgpt.monitor.TrainingMonitor object at '\n",
      "                     '0x15547caa3850>',\n",
      " 'use_pytorch_profiler': False}\n",
      "Time to instantiate model: 0.37 seconds.\n",
      "Total parameters: 1,425,294,848\n",
      "Training dataloader length: 307128\n",
      "Validation dataloader length: 312\n",
      "First batch of data: tensor([[  187,  1628,  1394,  ...,   608,     6,    32],\n",
      "        [13660,  6840,  2378,  ...,   187,   510,  6413],\n",
      "        [   14,    67, 23217,  ...,   281,  3693, 10305],\n",
      "        ...,\n",
      "        [31373,  3645,   310,  ...,  2458, 14549, 14092],\n",
      "        [  342,   731,    15,  ..., 14315,   326,  4433],\n",
      "        [ 8005,   386, 11558,  ...,  3282,    15,  6676]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "Shape of the first batch of data: torch.Size([16, 513])\n",
      "Verifying settings ...\n",
      "Measured TFLOPs: 61.15\n",
      "Info: Registered forward hook for module  \n",
      "Info: Registered forward hook for module  _forward_module\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.lm_head\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.wte\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.0\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.0.norm_1\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.0.attn\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.0.attn.attn\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.0.attn.proj\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.0.post_attention_norm\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.0.norm_2\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.0.mlp\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.0.mlp.fc\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.0.mlp.proj\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.0.post_mlp_norm\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.1\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.1.norm_1\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.1.attn\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.1.attn.attn\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.1.attn.proj\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.1.post_attention_norm\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.1.norm_2\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.1.mlp\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.1.mlp.fc\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.1.mlp.proj\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.1.post_mlp_norm\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.2\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.2.norm_1\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.2.attn\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.2.attn.attn\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.2.attn.proj\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.2.post_attention_norm\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.2.norm_2\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.2.mlp\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.2.mlp.fc\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.2.mlp.proj\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.2.post_mlp_norm\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.3\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.3.norm_1\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.3.attn\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.3.attn.attn\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.3.attn.proj\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.3.post_attention_norm\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.3.norm_2\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.3.mlp\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.3.mlp.fc\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.3.mlp.proj\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.3.post_mlp_norm\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.4\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.4.norm_1\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.4.attn\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.4.attn.attn\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.4.attn.proj\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.4.post_attention_norm\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.4.norm_2\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.4.mlp\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.4.mlp.fc\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.4.mlp.proj\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.4.post_mlp_norm\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.5\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.5.norm_1\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.5.attn\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.5.attn.attn\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.5.attn.proj\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.5.post_attention_norm\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.5.norm_2\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.5.mlp\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.5.mlp.fc\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.5.mlp.proj\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.h.5.post_mlp_norm\n",
      "Info: Registered forward hook for module  _forward_module._orig_mod.transformer.ln_f\n",
      "Saving checkpoint to '/weka/luxburg/sbordt10/moritz_sebastian_2025/moritz-pythia-14m-standard-width=4096-bf16mixed-lr=0.0001-warmup=700-timestamp=20250317_154603/step-00000000/lit_model.pth'\n",
      "Epoch 1 | iter 32 step 1 | loss train: 11.038, val: n/a | iter time: 225.81 ms (step) remaining time: 0:14:07\n",
      "Epoch 1 | iter 64 step 2 | loss train: 10.691, val: n/a | iter time: 152.15 ms (step) remaining time: 0:07:46\n",
      "Epoch 1 | iter 96 step 3 | loss train: 10.059, val: n/a | iter time: 150.23 ms (step) remaining time: 0:05:36\n",
      "Epoch 1 | iter 128 step 4 | loss train: 9.612, val: n/a | iter time: 151.83 ms (step) remaining time: 0:04:29\n",
      "Epoch 1 | iter 160 step 5 | loss train: 9.471, val: n/a | iter time: 151.01 ms (step) remaining time: 0:03:47\n",
      "Epoch 1 | iter 192 step 6 | loss train: 9.070, val: n/a | iter time: 151.11 ms (step) remaining time: 0:03:18\n",
      "Epoch 1 | iter 224 step 7 | loss train: 8.977, val: n/a | iter time: 151.79 ms (step) remaining time: 0:02:56\n",
      "Epoch 1 | iter 256 step 8 | loss train: 8.806, val: n/a | iter time: 151.07 ms (step) remaining time: 0:02:38\n",
      "Epoch 1 | iter 288 step 9 | loss train: 8.671, val: n/a | iter time: 152.79 ms (step) remaining time: 0:02:24\n",
      "Epoch 1 | iter 320 step 10 | loss train: 8.486, val: n/a | iter time: 153.03 ms (step) remaining time: 0:02:11\n",
      "Epoch 1 | iter 352 step 11 | loss train: 8.421, val: n/a | iter time: 152.02 ms (step) remaining time: 0:02:00\n",
      "Epoch 1 | iter 384 step 12 | loss train: 8.250, val: n/a | iter time: 151.19 ms (step) remaining time: 0:01:51\n",
      "Epoch 1 | iter 416 step 13 | loss train: 8.061, val: n/a | iter time: 151.04 ms (step) remaining time: 0:01:42\n",
      "Epoch 1 | iter 448 step 14 | loss train: 7.961, val: n/a | iter time: 150.21 ms (step) remaining time: 0:01:33\n",
      "Epoch 1 | iter 480 step 15 | loss train: 7.879, val: n/a | iter time: 151.71 ms (step) remaining time: 0:01:26\n",
      "Epoch 1 | iter 512 step 16 | loss train: 7.716, val: n/a | iter time: 152.32 ms (step) remaining time: 0:01:19\n",
      "Epoch 1 | iter 544 step 17 | loss train: 7.617, val: n/a | iter time: 150.38 ms (step) remaining time: 0:01:12\n",
      "Epoch 1 | iter 576 step 18 | loss train: 7.495, val: n/a | iter time: 151.35 ms (step) remaining time: 0:01:05\n",
      "Epoch 1 | iter 608 step 19 | loss train: 7.407, val: n/a | iter time: 151.51 ms (step) remaining time: 0:00:59\n",
      "Epoch 1 | iter 640 step 20 | loss train: 7.341, val: n/a | iter time: 151.99 ms (step) remaining time: 0:00:53\n",
      "Epoch 1 | iter 672 step 21 | loss train: 7.265, val: n/a | iter time: 152.69 ms (step) remaining time: 0:00:47\n",
      "Epoch 1 | iter 704 step 22 | loss train: 7.148, val: n/a | iter time: 153.02 ms (step) remaining time: 0:00:41\n",
      "Epoch 1 | iter 736 step 23 | loss train: 7.081, val: n/a | iter time: 153.42 ms (step) remaining time: 0:00:36\n",
      "Epoch 1 | iter 768 step 24 | loss train: 7.052, val: n/a | iter time: 151.97 ms (step) remaining time: 0:00:30\n",
      "Epoch 1 | iter 800 step 25 | loss train: 7.039, val: n/a | iter time: 152.62 ms (step) remaining time: 0:00:25\n",
      "Epoch 1 | iter 832 step 26 | loss train: 7.005, val: n/a | iter time: 151.57 ms (step) remaining time: 0:00:20\n",
      "Epoch 1 | iter 864 step 27 | loss train: 7.001, val: n/a | iter time: 154.56 ms (step) remaining time: 0:00:15\n",
      "Epoch 1 | iter 896 step 28 | loss train: 6.948, val: n/a | iter time: 151.06 ms (step) remaining time: 0:00:09\n",
      "Epoch 1 | iter 928 step 29 | loss train: 6.962, val: n/a | iter time: 150.63 ms (step) remaining time: 0:00:04\n",
      "Epoch 1 | iter 960 step 30 | loss train: 6.867, val: n/a | iter time: 149.90 ms (step) remaining time: 0:00:00\n",
      "Validating ...\n",
      "Final evaluation | val loss: 6.920 | val ppl: 1012.084\n",
      "Saving checkpoint to '/weka/luxburg/sbordt10/moritz_sebastian_2025/moritz-pythia-14m-standard-width=4096-bf16mixed-lr=0.0001-warmup=700-timestamp=20250317_154603/final/lit_model.pth'\n",
      "----------------------------------------\n",
      "| Performance\n",
      "| - Total tokens  : 7,864,320\n",
      "| - Training Time : 207.32 s\n",
      "| - Tok/sec       : 0.61 tok/s\n",
      "| ----------------------------------------\n",
      "| Memory Usage\n",
      "| - Memory Used   : 42.96 GB\n",
      "----------------------------------------\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mgallant-wood-301\u001b[0m at: \u001b[34mhttps://wandb.ai/mup_limitations/pretrain-pythia-14m/runs/31m0pahe\u001b[0m\n",
      "\n",
      "================================================================================\n",
      "================================================================================\n",
      "Content of logs/pythia-14m-mixed-lr-sweep/45282.err:\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: sbordt (train-on-test) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.8\n",
      "wandb: Run data is saved locally in ./wandb/run-20250317_154605-31m0pahe\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run gallant-wood-301\n",
      "wandb: ⭐️ View project at https://wandb.ai/mup_limitations/pretrain-pythia-14m\n",
      "wandb: 🚀 View run at https://wandb.ai/mup_limitations/pretrain-pythia-14m/runs/31m0pahe\n",
      "[rank: 0] Seed set to 42\n",
      "\n",
      "================================================================================\n",
      "================================================================================\n",
      "Content of logs/pythia-14m-mixed-lr-sweep/45270.out:\n",
      "JobId=45270 JobName=pythia-14m-mixed-lr-sweep\n",
      "   UserId=sbordt10(4198) GroupId=luxburg(4018) MCS_label=N/A\n",
      "   Priority=63351 Nice=0 Account=luxburg QOS=normal\n",
      "   JobState=RUNNING Reason=None Dependency=(null)\n",
      "   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n",
      "   RunTime=00:00:45 TimeLimit=3-00:00:00 TimeMin=N/A\n",
      "   SubmitTime=2025-03-17T14:59:11 EligibleTime=2025-03-17T14:59:11\n",
      "   AccrueTime=2025-03-17T14:59:11\n",
      "   StartTime=2025-03-17T14:59:11 EndTime=2025-03-20T14:59:11 Deadline=N/A\n",
      "   PreemptEligibleTime=2025-03-17T15:00:11 PreemptTime=None\n",
      "   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2025-03-17T14:59:11 Scheduler=Main\n",
      "   Partition=h100-ferranti AllocNode:Sid=ferranti-login001:1713349\n",
      "   ReqNodeList=(null) ExcNodeList=(null)\n",
      "   NodeList=mlcbm005\n",
      "   BatchHost=mlcbm005\n",
      "   NumNodes=1 NumCPUs=12 NumTasks=1 CPUs/Task=12 ReqB:S:C:T=0:0:*:*\n",
      "   ReqTRES=cpu=12,mem=128G,node=1,billing=102,gres/gpu=1\n",
      "   AllocTRES=cpu=12,mem=128G,node=1,billing=102,gres/gpu=1,gres/gpu:h100=1\n",
      "   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n",
      "   MinCPUsNode=12 MinMemoryNode=128G MinTmpDiskNode=0\n",
      "   Features=(null) DelayBoot=00:00:00\n",
      "   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n",
      "   Command=/weka/luxburg/sbordt10/limitations_of_tp_theory/scripts/ferranti/pythia-14m-mixed-lr-sweep.sh\n",
      "   WorkDir=/weka/luxburg/sbordt10/limitations_of_tp_theory/scripts/ferranti\n",
      "   StdErr=/weka/luxburg/sbordt10/logs/pythia-14m-mixed-lr-sweep/45270.err\n",
      "   StdIn=/dev/null\n",
      "   StdOut=/weka/luxburg/sbordt10/logs/pythia-14m-mixed-lr-sweep/45270.out\n",
      "   Power=\n",
      "   TresPerNode=gres/gpu:1\n",
      "   TresPerTask=cpu:12\n",
      "   \n",
      "\n",
      "Mon Mar 17 14:59:56 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  | 00000000:D7:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              69W / 700W |      4MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "Config(name='pythia-14m', hf_config={'org': 'EleutherAI', 'name': 'pythia-14m'}, scale_embeddings=False, attention_scores_scalar=None, block_size=512, sliding_window_size=None, sliding_window_layer_placing=None, vocab_size=50254, padding_multiple=128, padded_vocab_size=50304, n_layer=6, n_head=128, head_size=32, n_embd=4096, rotary_percentage=0.25, parallel_residual=True, bias=False, lm_head_bias=False, attn_bias=False, n_query_groups=4, shared_attention_norm=False, norm_class_name='RMSNorm', post_attention_norm=False, post_mlp_norm=False, norm_eps=1e-05, mlp_class_name='GptNeoxMLP', gelu_approximate='none', intermediate_size=16384, rope_condense_ratio=1, rope_base=10000, rope_adjustments=None, n_expert=0, n_expert_per_token=0, attention_logit_softcapping=None, final_logit_softcapping=None)\n",
      "SLURM_PROCID: 0, SLURM_NTASKS: 1\n",
      "d_model:  4096\n",
      "n_head:  128\n",
      "d_head:  32\n",
      "n_layer:  6\n",
      "MLP intermediate size:  16384\n",
      "Number of model parameters:  1425068032\n",
      "{'data': {},\n",
      " 'devices': 'auto',\n",
      " 'eval': {'evaluate_example': 'first',\n",
      "          'final_validation': True,\n",
      "          'initial_validation': False,\n",
      "          'interval': 1000,\n",
      "          'max_iters': 100000,\n",
      "          'max_new_tokens': None},\n",
      " 'get_lr_fn': None,\n",
      " 'initial_checkpoint_dir': None,\n",
      " 'initialize_weights_fn': '<function initialize_weights at 0x1555544cc9a0>',\n",
      " 'logger_kwargs': None,\n",
      " 'logger_name': 'wandb',\n",
      " 'model_config': {'attention_logit_softcapping': None,\n",
      "                  'attention_scores_scalar': None,\n",
      "                  'attn_bias': False,\n",
      "                  'bias': False,\n",
      "                  'block_size': 512,\n",
      "                  'final_logit_softcapping': None,\n",
      "                  'gelu_approximate': 'none',\n",
      "                  'head_size': 32,\n",
      "                  'hf_config': {'name': 'pythia-14m', 'org': 'EleutherAI'},\n",
      "                  'intermediate_size': 16384,\n",
      "                  'lm_head_bias': False,\n",
      "                  'mlp_class_name': 'GptNeoxMLP',\n",
      "                  'n_embd': 4096,\n",
      "                  'n_expert': 0,\n",
      "                  'n_expert_per_token': 0,\n",
      "                  'n_head': 128,\n",
      "                  'n_layer': 6,\n",
      "                  'n_query_groups': 4,\n",
      "                  'name': 'pythia-14m',\n",
      "                  'norm_class_name': 'RMSNorm',\n",
      "                  'norm_eps': 1e-05,\n",
      "                  'padded_vocab_size': 50304,\n",
      "                  'padding_multiple': 128,\n",
      "                  'parallel_residual': True,\n",
      "                  'post_attention_norm': False,\n",
      "                  'post_mlp_norm': False,\n",
      "                  'rope_adjustments': None,\n",
      "                  'rope_base': 10000,\n",
      "                  'rope_condense_ratio': 1,\n",
      "                  'rotary_percentage': 0.25,\n",
      "                  'scale_embeddings': False,\n",
      "                  'shared_attention_norm': False,\n",
      "                  'sliding_window_layer_placing': None,\n",
      "                  'sliding_window_size': None,\n",
      "                  'vocab_size': 50254},\n",
      " 'model_name': None,\n",
      " 'num_nodes': 1,\n",
      " 'optimizer': \"{'class_path': 'torch.optim.AdamW', 'init_args': {'lr': 0.0001, \"\n",
      "              \"'betas': (0.9, 0.95), 'weight_decay': 0.1}}\",\n",
      " 'out_dir': '/weka/luxburg/sbordt10/moritz_sebastian_2025/moritz-pythia-14m-standard-width=4096-bf16mixed-lr=0.0001-warmup=700-timestamp=20250317_150005',\n",
      " 'precision': 'bf16-mixed',\n",
      " 'resume': False,\n",
      " 'seed': 42,\n",
      " 'tokenizer_dir': PosixPath('checkpoints/EleutherAI/pythia-14m'),\n",
      " 'train': {'epochs': None,\n",
      "           'global_batch_size': 512,\n",
      "           'log_interval': 1,\n",
      "           'lr_warmup_fraction': None,\n",
      "           'lr_warmup_steps': 700.0,\n",
      "           'max_norm': 1.0,\n",
      "           'max_seq_length': 512,\n",
      "           'max_steps': None,\n",
      "           'max_tokens': 7864320,\n",
      "           'micro_batch_size': 16,\n",
      "           'min_lr': 1e-05,\n",
      "           'save_interval': 1000,\n",
      "           'tie_embeddings': False},\n",
      " 'training_monitor': '<litgpt.monitor.TrainingMonitor object at '\n",
      "                     '0x15547cb5e790>',\n",
      " 'use_pytorch_profiler': False}\n",
      "Time to instantiate model: 0.26 seconds.\n",
      "Total parameters: 1,425,068,032\n",
      "Training dataloader length: 307128\n",
      "Validation dataloader length: 312\n",
      "First batch of data: tensor([[  187,  1628,  1394,  ...,   608,     6,    32],\n",
      "        [13660,  6840,  2378,  ...,   187,   510,  6413],\n",
      "        [   14,    67, 23217,  ...,   281,  3693, 10305],\n",
      "        ...,\n",
      "        [31373,  3645,   310,  ...,  2458, 14549, 14092],\n",
      "        [  342,   731,    15,  ..., 14315,   326,  4433],\n",
      "        [ 8005,   386, 11558,  ...,  3282,    15,  6676]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "Shape of the first batch of data: torch.Size([16, 513])\n",
      "Verifying settings ...\n",
      "Measured TFLOPs: 61.15\n",
      "Saving checkpoint to '/weka/luxburg/sbordt10/moritz_sebastian_2025/moritz-pythia-14m-standard-width=4096-bf16mixed-lr=0.0001-warmup=700-timestamp=20250317_150005/step-00000000/lit_model.pth'\n",
      "Epoch 1 | iter 32 step 1 | loss train: 11.038, val: n/a | iter time: 222.56 ms (step) remaining time: 0:12:57\n",
      "Epoch 1 | iter 64 step 2 | loss train: 10.691, val: n/a | iter time: 149.90 ms (step) remaining time: 0:07:11\n",
      "Epoch 1 | iter 96 step 3 | loss train: 10.059, val: n/a | iter time: 148.20 ms (step) remaining time: 0:05:13\n",
      "Epoch 1 | iter 128 step 4 | loss train: 9.614, val: n/a | iter time: 147.20 ms (step) remaining time: 0:04:12\n",
      "Epoch 1 | iter 160 step 5 | loss train: 9.471, val: n/a | iter time: 149.39 ms (step) remaining time: 0:03:34\n",
      "Epoch 1 | iter 192 step 6 | loss train: 9.070, val: n/a | iter time: 149.47 ms (step) remaining time: 0:03:07\n",
      "Epoch 1 | iter 224 step 7 | loss train: 8.978, val: n/a | iter time: 149.24 ms (step) remaining time: 0:02:47\n",
      "Epoch 1 | iter 256 step 8 | loss train: 8.806, val: n/a | iter time: 150.40 ms (step) remaining time: 0:02:30\n",
      "Epoch 1 | iter 288 step 9 | loss train: 8.671, val: n/a | iter time: 148.04 ms (step) remaining time: 0:02:17\n",
      "Epoch 1 | iter 320 step 10 | loss train: 8.486, val: n/a | iter time: 150.61 ms (step) remaining time: 0:02:05\n",
      "Epoch 1 | iter 352 step 11 | loss train: 8.422, val: n/a | iter time: 149.92 ms (step) remaining time: 0:01:55\n",
      "Epoch 1 | iter 384 step 12 | loss train: 8.249, val: n/a | iter time: 148.49 ms (step) remaining time: 0:01:46\n",
      "Epoch 1 | iter 416 step 13 | loss train: 8.061, val: n/a | iter time: 151.25 ms (step) remaining time: 0:01:38\n",
      "Epoch 1 | iter 448 step 14 | loss train: 7.961, val: n/a | iter time: 150.00 ms (step) remaining time: 0:01:30\n",
      "Epoch 1 | iter 480 step 15 | loss train: 7.880, val: n/a | iter time: 148.47 ms (step) remaining time: 0:01:23\n",
      "Epoch 1 | iter 512 step 16 | loss train: 7.717, val: n/a | iter time: 148.72 ms (step) remaining time: 0:01:16\n",
      "Epoch 1 | iter 544 step 17 | loss train: 7.617, val: n/a | iter time: 149.47 ms (step) remaining time: 0:01:09\n",
      "Epoch 1 | iter 576 step 18 | loss train: 7.494, val: n/a | iter time: 151.55 ms (step) remaining time: 0:01:03\n",
      "Epoch 1 | iter 608 step 19 | loss train: 7.407, val: n/a | iter time: 148.85 ms (step) remaining time: 0:00:57\n",
      "Epoch 1 | iter 640 step 20 | loss train: 7.341, val: n/a | iter time: 148.40 ms (step) remaining time: 0:00:51\n",
      "Epoch 1 | iter 672 step 21 | loss train: 7.266, val: n/a | iter time: 148.23 ms (step) remaining time: 0:00:45\n",
      "Epoch 1 | iter 704 step 22 | loss train: 7.147, val: n/a | iter time: 151.07 ms (step) remaining time: 0:00:40\n",
      "Epoch 1 | iter 736 step 23 | loss train: 7.080, val: n/a | iter time: 150.29 ms (step) remaining time: 0:00:35\n",
      "Epoch 1 | iter 768 step 24 | loss train: 7.050, val: n/a | iter time: 145.28 ms (step) remaining time: 0:00:29\n",
      "Epoch 1 | iter 800 step 25 | loss train: 7.037, val: n/a | iter time: 149.22 ms (step) remaining time: 0:00:24\n",
      "Epoch 1 | iter 832 step 26 | loss train: 6.992, val: n/a | iter time: 149.04 ms (step) remaining time: 0:00:19\n",
      "Epoch 1 | iter 864 step 27 | loss train: 6.981, val: n/a | iter time: 148.36 ms (step) remaining time: 0:00:14\n",
      "Epoch 1 | iter 896 step 28 | loss train: 6.966, val: n/a | iter time: 148.79 ms (step) remaining time: 0:00:09\n",
      "Epoch 1 | iter 928 step 29 | loss train: 6.990, val: n/a | iter time: 148.83 ms (step) remaining time: 0:00:04\n",
      "Epoch 1 | iter 960 step 30 | loss train: 6.886, val: n/a | iter time: 148.03 ms (step) remaining time: 0:00:00\n",
      "Validating ...\n",
      "Final evaluation | val loss: 7.118 | val ppl: 1233.595\n",
      "Saving checkpoint to '/weka/luxburg/sbordt10/moritz_sebastian_2025/moritz-pythia-14m-standard-width=4096-bf16mixed-lr=0.0001-warmup=700-timestamp=20250317_150005/final/lit_model.pth'\n",
      "----------------------------------------\n",
      "| Performance\n",
      "| - Total tokens  : 7,864,320\n",
      "| - Training Time : 194.80 s\n",
      "| - Tok/sec       : 0.61 tok/s\n",
      "| ----------------------------------------\n",
      "| Memory Usage\n",
      "| - Memory Used   : 42.96 GB\n",
      "----------------------------------------\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33melectric-sun-300\u001b[0m at: \u001b[34mhttps://wandb.ai/mup_limitations/pretrain-pythia-14m/runs/n5z2vlgf\u001b[0m\n",
      "\n",
      "================================================================================\n",
      "================================================================================\n",
      "Content of logs/pythia-14m-mixed-lr-sweep/45270.err:\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: sbordt (train-on-test) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.8\n",
      "wandb: Run data is saved locally in ./wandb/run-20250317_150007-n5z2vlgf\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run electric-sun-300\n",
      "wandb: ⭐️ View project at https://wandb.ai/mup_limitations/pretrain-pythia-14m\n",
      "wandb: 🚀 View run at https://wandb.ai/mup_limitations/pretrain-pythia-14m/runs/n5z2vlgf\n",
      "[rank: 0] Seed set to 42\n",
      "\n",
      "================================================================================\n",
      "================================================================================\n",
      "Content of logs/pythia-14m-mixed-lr-sweep/45263.err:\n",
      "Traceback (most recent call last):\n",
      "  File \"/weka/luxburg/sbordt10/limitations_of_tp_theory/standard_training_moritz.py\", line 81, in <module>\n",
      "    parser.add_argument(\"--bias_profiling\", type=bool, action=\"store_true\", default=False)\n",
      "  File \"/weka/luxburg/sbordt10/anaconda3/envs/tp-theory-new/lib/python3.11/argparse.py\", line 1455, in add_argument\n",
      "    action = action_class(**kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: _StoreTrueAction.__init__() got an unexpected keyword argument 'type'\n",
      "\n",
      "================================================================================\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "ferranti_print_logs(\"logs/pythia-14m-mixed-lr-sweep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 423faa0..eaf6509\n",
      "Fast-forward\n",
      " extensions/thunder/pretrain.py             |   2 +-\n",
      " litgpt/config.py                           |  11 +-\n",
      " litgpt/finetune/adapter.py                 |   2 +-\n",
      " litgpt/finetune/adapter_v2.py              |   2 +-\n",
      " litgpt/finetune/full.py                    |   2 +-\n",
      " litgpt/finetune/lora.py                    |   2 +-\n",
      " litgpt/model.py                            |  18 +-\n",
      " litgpt/monitor.py                          | 554 +++++++++++++++++------------\n",
      " litgpt/pretrain.py                         | 253 ++++++++-----\n",
      " litgpt/utils.py                            |   4 +-\n",
      " pretrain-experiment/DclmData.py            |  75 ++++\n",
      " pretrain-experiment/pretrain-experiment.py | 273 ++++++++++++++\n",
      " pretrain-experiment/project_utils.py       |  71 ++++\n",
      " tests/test_utils.py                        |   8 +-\n",
      " 14 files changed, 949 insertions(+), 328 deletions(-)\n",
      " create mode 100644 pretrain-experiment/DclmData.py\n",
      " create mode 100644 pretrain-experiment/pretrain-experiment.py\n",
      " create mode 100644 pretrain-experiment/project_utils.py\n",
      "\n",
      "From github.com:sbordt/litgpt\n",
      "   423faa0..eaf6509  main                    -> origin/main\n",
      "   dab1c4c..80f11a5  torch.compile-and-hooks -> origin/torch.compile-and-hooks\n",
      "\n",
      "Updating ae9dbc8..8a4a7d4\n",
      "Fast-forward\n",
      " .gitignore                                         |     2 +-\n",
      " DclmData.py                                        |    75 -\n",
      " LegacyDclmData.py                                  |    71 -\n",
      " cancel_jobs.sh                                     |    14 -\n",
      " cleanup-litgpt-checkpoints.py                      |   102 +\n",
      " control_galvani.ipynb                              |  2960 -----\n",
      " dclm-baseline-1.0-urls-part1.txt                   |   500 -\n",
      " dclm-baseline-1.0-urls-part2.txt                   |   500 -\n",
      " dclm_baseline_shard_order.pkl                      |   Bin 696195 -> 0 bytes\n",
      " mlcloud.py                                         |     6 +-\n",
      " .../contamination.ipynb                            |     0\n",
      " .../contorl_ferranti.ipynb                         |    93 +-\n",
      " notebooks/control_galvani.ipynb                    |  2173 ++++\n",
      " .../coordinate-check.ipynb                         |     0\n",
      " notebooks/dclm-baseline.ipynb                      |   711 ++\n",
      " .../design_experiments.ipynb                       |     0\n",
      " .../inspect_pre_training_data.ipynb                |     0\n",
      " .../profile_training.ipynb                         |    14 +-\n",
      " .../results-from-wandb.ipynb                       |   259 +-\n",
      " notebooks/simple-training-loop.ipynb               | 10659 +++++++++++++++++++\n",
      " profile_training.py                                |   183 -\n",
      " run_a100_sweep.sh                                  |    17 -\n",
      " .../coordinate_check/standard-transformer.sh       |    44 +\n",
      " scripts/galvani/lr_sweeps/SGD-lr-sweep-float32.sh  |    51 +\n",
      " scripts/galvani/lr_sweeps/SGD-lr-sweep.sh          |    52 +\n",
      " .../{ => old_scripts}/pythia-14m-mixed-lr-sweep.sh |     0\n",
      " .../pythia-14m-width=1024-bf16-lr-sweep.sh         |     0\n",
      " .../pythia-14m-width=256-bf16-lr-sweep.sh          |     0\n",
      " .../pythia-14m-width=4096-bf16-lr-sweep.sh         |     0\n",
      " .../resume-pythia-14m-mixed-lr-sweep.sh            |     0\n",
      " .../standard-transformer-lr-sweep-7B-tokens.sh     |    49 +\n",
      " .../lr_sweeps/standard-transformer-lr-sweep.sh     |    50 +\n",
      " standard_training.py                               |   194 -\n",
      " standard_training_moritz.py                        |   196 -\n",
      " unit-scaling-demo.ipynb                            |   635 --\n",
      " 35 files changed, 14241 insertions(+), 5369 deletions(-)\n",
      " delete mode 100644 DclmData.py\n",
      " delete mode 100644 LegacyDclmData.py\n",
      " delete mode 100755 cancel_jobs.sh\n",
      " create mode 100644 cleanup-litgpt-checkpoints.py\n",
      " delete mode 100644 control_galvani.ipynb\n",
      " delete mode 100644 dclm-baseline-1.0-urls-part1.txt\n",
      " delete mode 100644 dclm-baseline-1.0-urls-part2.txt\n",
      " delete mode 100644 dclm_baseline_shard_order.pkl\n",
      " rename contamination.ipynb => notebooks/contamination.ipynb (100%)\n",
      " rename contorl_ferranti.ipynb => notebooks/contorl_ferranti.ipynb (96%)\n",
      " create mode 100644 notebooks/control_galvani.ipynb\n",
      " rename coordinate-check.ipynb => notebooks/coordinate-check.ipynb (100%)\n",
      " create mode 100644 notebooks/dclm-baseline.ipynb\n",
      " rename design_experiments.ipynb => notebooks/design_experiments.ipynb (100%)\n",
      " rename inspect_pre_training_data.ipynb => notebooks/inspect_pre_training_data.ipynb (100%)\n",
      " rename profile_training.ipynb => notebooks/profile_training.ipynb (99%)\n",
      " rename results-from-wandb.ipynb => notebooks/results-from-wandb.ipynb (51%)\n",
      " create mode 100644 notebooks/simple-training-loop.ipynb\n",
      " delete mode 100644 profile_training.py\n",
      " delete mode 100755 run_a100_sweep.sh\n",
      " create mode 100644 scripts/ferranti/coordinate_check/standard-transformer.sh\n",
      " create mode 100644 scripts/galvani/lr_sweeps/SGD-lr-sweep-float32.sh\n",
      " create mode 100644 scripts/galvani/lr_sweeps/SGD-lr-sweep.sh\n",
      " rename scripts/galvani/lr_sweeps/{ => old_scripts}/pythia-14m-mixed-lr-sweep.sh (100%)\n",
      " rename scripts/galvani/lr_sweeps/{ => old_scripts}/pythia-14m-width=1024-bf16-lr-sweep.sh (100%)\n",
      " rename scripts/galvani/lr_sweeps/{ => old_scripts}/pythia-14m-width=256-bf16-lr-sweep.sh (100%)\n",
      " rename scripts/galvani/lr_sweeps/{ => old_scripts}/pythia-14m-width=4096-bf16-lr-sweep.sh (100%)\n",
      " rename scripts/galvani/lr_sweeps/{ => old_scripts}/resume-pythia-14m-mixed-lr-sweep.sh (100%)\n",
      " create mode 100644 scripts/galvani/lr_sweeps/standard-transformer-lr-sweep-7B-tokens.sh\n",
      " create mode 100644 scripts/galvani/lr_sweeps/standard-transformer-lr-sweep.sh\n",
      " delete mode 100644 standard_training.py\n",
      " delete mode 100644 standard_training_moritz.py\n",
      " delete mode 100644 unit-scaling-demo.ipynb\n",
      "\n",
      "From github.com:sbordt/limitations_of_tp_theory\n",
      "   ae9dbc8..8a4a7d4  main       -> origin/main\n",
      "   ae9dbc8..339f1e9  dev        -> origin/dev\n",
      " * [new branch]      experiment-no-layer-norm -> origin/experiment-no-layer-norm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ferranti_exec(\"cd litgpt && git pull\")\n",
    "ferranti_exec(\"cd limitations_of_tp_theory && git pull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "error: pathspec 'torch.compile-and-hooks' did not match any file(s) known to git\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ferranti_exec(\"cd limitations_of_tp_theory  && git checkout torch.compile-and-hooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "             45282 h100-ferr pythia-1 sbordt10  R       3:26      1 mlcbm005\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ferranti_exec(\"squeue --me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST          START_TIME  NODES SCHEDNODES           NODELIST(REASON)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ferranti_exec(\"squeue -u sbordt10  --start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cancel a specific job\n",
    "ferranti_exec(\"scancel 45262\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 42263\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ferranti_exec_H100(\"conda create -n tp-theory-new python=3.11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 42264\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ferranti_exec_H100(\"conda init && activate tp-theory-new && pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 42265\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ferranti_exec_H100(\"source activate tp-theory-new && pip install wandb tenacity datasets h5py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 42266\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ferranti_exec_H100(\"source activate tp-theory-new && pip install litdata==0.2.28\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 42267\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ferranti_exec_H100(\"source activate tp-theory-new && git clone git@github.com:sbordt/litgpt.git && cd litgpt && pip install -e .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 42268\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ferranti_exec_H100(\"source activate tp-theory-new && git clone git@github.com:sbordt/limitations_of_tp_theory.git\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 42269\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ferranti_exec_H100(\"source activate tp-theory-new && cd limitations_of_tp_theory && litgpt download EleutherAI/pythia-14m --tokenizer_only true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/weka/luxburg/sbordt10\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ferranti_exec(\"pwd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdamW Coordinate check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 52312\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lr that is optimal for widht=256 (lr = 0.01) for all widhts\n",
    "lr = 0.01\n",
    "\n",
    "for width in [4096]:\n",
    "    ferranti_exec(f\"cd /weka/luxburg/sbordt10/limitations_of_tp_theory/scripts/ferranti/coordinate_check && sbatch standard-transformer-coordinate-check-init.sh {lr} {width} 700 4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 52313\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lr*=(256/width)\n",
    "lr_base = 0.01\n",
    "\n",
    "for width in [4096]:\n",
    "    lr = lr_base * (256. / width)\n",
    "    ferranti_exec(f\"cd /weka/luxburg/sbordt10/limitations_of_tp_theory/scripts/ferranti/coordinate_check && sbatch standard-transformer-coordinate-check-init.sh {lr} {width} 700 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Jobs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download dclm baseline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ferranti_exec_cpu(\"cd limitations_of_tp_theory && python batch-downloader.py -o /weka/luxburg/sbordt10/dclm-baseline-1.0/part1 -f dclm-baseline-1.0-urls-part1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 43149\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ferranti_exec_cpu(\"cd limitations_of_tp_theory && python tokenize_dclm_baseline.py --data_files '/weka/luxburg/sbordt10/dclm-baseline-1.0/part1/*.parquet' --output_dir '/weka/luxburg/sbordt10/dclm-baseline-1.0-tokenized/part1-train/' --tokenizer 'checkpoints/EleutherAI/pythia-14m' --validation_output_dir '/weka/luxburg/sbordt10/dclm-baseline-1.0-tokenized/part1-val/' --num_workers 32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compress tokenized data for download to galvani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 45867\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ferranti_exec_cpu(\"tar -czvf /home/luxburg/sbordt10/dclm-baseline-1.0-tokenized-preview.tar.gz /home/luxburg/sbordt10/dclm-baseline-1.0-tokenized-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pythia-14m widh=4096 lr sweep on a single H100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## development and debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating f7ee1f9..a25f0ec\n",
      "Fast-forward\n",
      " litgpt/monitor.py                          |  2 --\n",
      " litgpt/pretrain.py                         | 10 ++++++++++\n",
      " pretrain-experiment/pretrain-experiment.py |  3 +++\n",
      " 3 files changed, 13 insertions(+), 2 deletions(-)\n",
      "\n",
      "From github.com:sbordt/litgpt\n",
      "   f7ee1f9..a25f0ec  main       -> origin/main\n",
      "\n",
      "Updating 8a4a7d4..217d220\n",
      "Fast-forward\n",
      " notebooks/contorl_ferranti.ipynb                   | 1579 +++++++++++---------\n",
      " notebooks/control_galvani.ipynb                    |  242 +--\n",
      " notebooks/simple-training-loop.ipynb               |  115 +-\n",
      " ... standard-transformer-coordinate-check-init.sh} |   11 +-\n",
      " .../standard-transformer-coordinate-check-init.sh  |   48 +\n",
      " 5 files changed, 1164 insertions(+), 831 deletions(-)\n",
      " rename scripts/ferranti/coordinate_check/{standard-transformer.sh => standard-transformer-coordinate-check-init.sh} (83%)\n",
      " create mode 100644 scripts/galvani/coordinate_check/standard-transformer-coordinate-check-init.sh\n",
      "\n",
      "From github.com:sbordt/limitations_of_tp_theory\n",
      "   8a4a7d4..217d220  main       -> origin/main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ferranti_exec(\"cd /home/luxburg/sbordt10/litgpt && git pull\")\n",
    "ferranti_exec(\"cd /home/luxburg/sbordt10/limitations_of_tp_theory && git pull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of /weka/luxburg/sbordt10/logs/exec_h100/52165.err:\n",
      "INFO:root:Run directory: /home/luxburg/sbordt10/moritz_sebastian_2025/development and debugging/pretrain-pythia-14m-id=20250422205505\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: sbordt (train-on-test) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: Tracking run with wandb version 0.19.8\n",
      "wandb: Run data is saved locally in ./wandb/run-20250422_205512-utb6tl6p\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run pretrain-pythia-14m-id=20250422205505\n",
      "wandb: ⭐️ View project at https://wandb.ai/mup_limitations/development%20and%20debugging\n",
      "wandb: 🚀 View run at https://wandb.ai/mup_limitations/development%20and%20debugging/runs/utb6tl6p\n",
      "[rank: 0] Seed set to 42\n",
      "INFO:ModuleMonitor:Logged 14905 keys at step 1. Total size of log data: 0.00 MB\n",
      "INFO:ModuleMonitor:Logged 14905 keys at step 2. Total size of log data: 0.00 MB\n",
      "INFO:ModuleMonitor:Logged 14905 keys at step 3. Total size of log data: 0.00 MB\n",
      "INFO:ModuleMonitor:Logged 14905 keys at step 4. Total size of log data: 0.00 MB\n",
      "INFO:ModuleMonitor:Logged 14905 keys at step 5. Total size of log data: 0.00 MB\n",
      "INFO:ModuleMonitor:Logged 14905 keys at step 6. Total size of log data: 0.00 MB\n",
      "INFO:ModuleMonitor:Logged 14905 keys at step 7. Total size of log data: 0.00 MB\n",
      "INFO:ModuleMonitor:Logged 14905 keys at step 8. Total size of log data: 0.00 MB\n",
      "INFO:ModuleMonitor:Logged 14905 keys at step 9. Total size of log data: 0.00 MB\n",
      "INFO:ModuleMonitor:Logged 14905 keys at step 10. Total size of log data: 0.00 MB\n",
      "INFO:ModuleMonitor:Logged 14905 keys at step 11. Total size of log data: 0.00 MB\n",
      "INFO:ModuleMonitor:Logged 14905 keys at step 12. Total size of log data: 0.00 MB\n",
      "INFO:ModuleMonitor:Logged 14905 keys at step 13. Total size of log data: 0.00 MB\n",
      "INFO:ModuleMonitor:Logged 14905 keys at step 14. Total size of log data: 0.00 MB\n",
      "INFO:ModuleMonitor:Logged 14905 keys at step 15. Total size of log data: 0.00 MB\n",
      "INFO:ModuleMonitor:Logged 14905 keys at step 16. Total size of log data: 0.00 MB\n",
      "slurmstepd: error: *** JOB 52165 ON mlcbm006 CANCELLED AT 2025-04-22T21:55:23 DUE TO TIME LIMIT ***\n",
      "\n",
      "================================================================================\n",
      "================================================================================\n",
      "Content of /weka/luxburg/sbordt10/logs/exec_h100/52165.out:\n",
      "JobId=52165 JobName=ferranti-exec-h100\n",
      "   UserId=sbordt10(4198) GroupId=luxburg(4018) MCS_label=N/A\n",
      "   Priority=70846 Nice=0 Account=luxburg QOS=normal\n",
      "   JobState=RUNNING Reason=None Dependency=(null)\n",
      "   Requeue=1 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n",
      "   RunTime=00:00:08 TimeLimit=01:00:00 TimeMin=N/A\n",
      "   SubmitTime=2025-04-22T20:54:52 EligibleTime=2025-04-22T20:54:52\n",
      "   AccrueTime=2025-04-22T20:54:53\n",
      "   StartTime=2025-04-22T20:54:53 EndTime=2025-04-22T21:54:53 Deadline=N/A\n",
      "   PreemptEligibleTime=2025-04-22T20:55:53 PreemptTime=None\n",
      "   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2025-04-22T20:54:53 Scheduler=Main\n",
      "   Partition=h100-ferranti AllocNode:Sid=ferranti-login001:778724\n",
      "   ReqNodeList=(null) ExcNodeList=(null)\n",
      "   NodeList=mlcbm006\n",
      "   BatchHost=mlcbm006\n",
      "   NumNodes=1 NumCPUs=8 NumTasks=1 CPUs/Task=8 ReqB:S:C:T=0:0:*:*\n",
      "   ReqTRES=cpu=8,mem=128G,node=1,billing=102,gres/gpu=1\n",
      "   AllocTRES=cpu=8,mem=128G,node=1,billing=102,gres/gpu=1,gres/gpu:h100=1\n",
      "   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n",
      "   MinCPUsNode=8 MinMemoryNode=128G MinTmpDiskNode=0\n",
      "   Features=(null) DelayBoot=00:00:00\n",
      "   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n",
      "   Command=(null)\n",
      "   WorkDir=/weka/luxburg/sbordt10\n",
      "   StdErr=/weka/luxburg/sbordt10/logs/exec_h100/52165.err\n",
      "   StdIn=/dev/null\n",
      "   StdOut=/weka/luxburg/sbordt10/logs/exec_h100/52165.out\n",
      "   Power=\n",
      "   TresPerNode=gres/gpu:1\n",
      "   TresPerTask=cpu:8\n",
      "   \n",
      "\n",
      "Tue Apr 22 20:55:01 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA H100 80GB HBM3          On  | 00000000:84:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              71W / 700W |      4MiB / 81559MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "SLURM_PROCID: 0, SLURM_NTASKS: 1\n",
      "d_model:  4096\n",
      "n_head:  128\n",
      "d_head:  32\n",
      "n_layer:  6\n",
      "MLP intermediate size:  16384\n",
      "Number of model parameters:  1425295616\n",
      "{'auto_cancel': False,\n",
      " 'data': {},\n",
      " 'devices': 'auto',\n",
      " 'eval': {'evaluate_example': 'first',\n",
      "          'final_validation': True,\n",
      "          'initial_validation': False,\n",
      "          'interval': 1000,\n",
      "          'max_iters': 100000,\n",
      "          'max_new_tokens': None},\n",
      " 'get_lr_fn': '<function get_lr at 0x1555554be0c0>',\n",
      " 'initial_checkpoint_dir': None,\n",
      " 'initialize_weights_fn': '<function initialize_standard_weights at '\n",
      "                          '0x15548572f880>',\n",
      " 'logger_kwargs': \"{'name': 'pretrain-pythia-14m-id=20250422205505', \"\n",
      "                  \"'project': 'development and debugging'}\",\n",
      " 'logger_name': 'wandb',\n",
      " 'model_config': {'attention_logit_softcapping': None,\n",
      "                  'attention_scores_scalar': None,\n",
      "                  'attn_bias': False,\n",
      "                  'bias': True,\n",
      "                  'block_size': 512,\n",
      "                  'final_logit_softcapping': None,\n",
      "                  'gelu_approximate': 'none',\n",
      "                  'head_size': 32,\n",
      "                  'hf_config': {'name': 'pythia-14m', 'org': 'EleutherAI'},\n",
      "                  'intermediate_size': 16384,\n",
      "                  'lm_head_bias': False,\n",
      "                  'mlp_class_name': 'GptNeoxMLP',\n",
      "                  'n_embd': 4096,\n",
      "                  'n_expert': 0,\n",
      "                  'n_expert_per_token': 0,\n",
      "                  'n_head': 128,\n",
      "                  'n_layer': 6,\n",
      "                  'n_query_groups': 4,\n",
      "                  'name': 'pythia-14m',\n",
      "                  'norm_class_name': 'LayerNorm',\n",
      "                  'norm_eps': 1e-05,\n",
      "                  'padded_vocab_size': 50304,\n",
      "                  'padding_multiple': 128,\n",
      "                  'parallel_residual': True,\n",
      "                  'post_attention_norm': False,\n",
      "                  'post_mlp_norm': False,\n",
      "                  'qk_norm': True,\n",
      "                  'rope_adjustments': None,\n",
      "                  'rope_base': 10000,\n",
      "                  'rope_condense_ratio': 1,\n",
      "                  'rotary_percentage': 0.25,\n",
      "                  'scale_embeddings': False,\n",
      "                  'shared_attention_norm': False,\n",
      "                  'sliding_window_layer_placing': None,\n",
      "                  'sliding_window_size': None,\n",
      "                  'vocab_size': 50254},\n",
      " 'model_name': None,\n",
      " 'num_nodes': 1,\n",
      " 'optimizer': \"{'class_path': 'torch.optim.AdamW', 'init_args': {'lr': 0.001, \"\n",
      "              \"'betas': (0.9, 0.95), 'weight_decay': 0.1}}\",\n",
      " 'out_dir': '/home/luxburg/sbordt10/moritz_sebastian_2025/development and '\n",
      "            'debugging/pretrain-pythia-14m-id=20250422205505',\n",
      " 'precision': 'bf16-mixed',\n",
      " 'reference_model_type': 'init',\n",
      " 'resume': False,\n",
      " 'seed': 42,\n",
      " 'tokenizer_dir': None,\n",
      " 'train': {'epochs': None,\n",
      "           'global_batch_size': 512,\n",
      "           'log_interval': 1,\n",
      "           'lr_warmup_fraction': None,\n",
      "           'lr_warmup_steps': 700,\n",
      "           'max_norm': 1.0,\n",
      "           'max_seq_length': 512,\n",
      "           'max_steps': None,\n",
      "           'max_tokens': 15728640,\n",
      "           'micro_batch_size': 2,\n",
      "           'min_lr': 0.0001,\n",
      "           'save_interval': 50,\n",
      "           'tie_embeddings': False},\n",
      " 'training_monitor': '<litgpt.monitor.ModuleMonitor object at 0x1554852d0550>',\n",
      " 'use_pytorch_profiler': False,\n",
      " 'with_activation_differences': True,\n",
      " 'with_compile': False,\n",
      " 'with_mup_coordinate_check': True}\n",
      "Time to instantiate model: 0.25 seconds.\n",
      "Total parameters: 1,425,295,616\n",
      "Training dataloader length: 65844528\n",
      "Validation dataloader length: 2528\n",
      "First batch of data: tensor([[ 6406,    32,   187,  ..., 30156,  1057,  1066],\n",
      "        [   13,   556,   642,  ...,   253, 37527,    15]], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "Shape of the first batch of data: torch.Size([2, 513])\n",
      "Memory allocated: 11.40 GB\n",
      "Max memory allocated: 11.40 GB\n",
      "Memory allocated: 11.40 GB\n",
      "Max memory allocated: 11.40 GB\n",
      "Verifying settings ...\n",
      "Measured TFLOPs: 7.64\n",
      "\n",
      "================================================================================\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "ferranti_print_logs(\"/weka/luxburg/sbordt10/logs/exec_h100/\", num_files=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n",
      "\n",
      "\n",
      "Already up to date.\n",
      "\n",
      "\n",
      "Submitted batch job 52264\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ferranti_exec(\"cd /home/luxburg/sbordt10/litgpt && git pull\")\n",
    "ferranti_exec(\"cd /home/luxburg/sbordt10/limitations_of_tp_theory && git pull\")\n",
    "\n",
    "ferranti_exec_H100(\"\"\"cd /home/luxburg/sbordt10/litgpt/pretrain-experiment && python pretrain-experiment.py \\\n",
    "    --experiment_name \"development and debugging\" \\\n",
    "    --output_dir \"/home/luxburg/sbordt10/moritz_sebastian_2025\"   \\\n",
    "    --data_dir \"/home/luxburg/sbordt10/dclm-baseline-1.0-tokenized\" \\\n",
    "    --save_interval 50 \\\n",
    "    --model \"pythia-14m\" \\\n",
    "    --width 4096 \\\n",
    "    --max_tokens 15728640 \\\n",
    "    --max_seq_length 512 \\\n",
    "    --global_batch_size 256 \\\n",
    "    --micro_batch_size 4 \\\n",
    "    --lr 0.01 \\\n",
    "    --warmup_steps 700 \\\n",
    "    --precision \"bf16-mixed\" \\\n",
    "    --seed 42 \\\n",
    "    --qk_norm \\\n",
    "    --reference_model \"init\" \\\n",
    "    --mup_coordinate_check \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "             52313 h100-ferr coordina sbordt10 PD       0:00      1 (Resources)\n",
      "             52312 h100-ferr coordina sbordt10  R       8:27      1 mlcbm007\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ferranti_exec(\"squeue --me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olmo-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
